{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python Business Analytics.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMMcEj0sqoNlGG8WMY0ewat",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xwang222/My-Projects/blob/main/Python_Business_Analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python Data Analytics\n",
        "\n",
        "This notebook is a summary of codes commonly used for data cleaning and descriptive analysis, which is summartized across several fantastic notebooks here:\n",
        "\n",
        "https://github.com/firmai/python-business-analytics\n",
        "\n",
        "For detailed implementation and better demostration, please visit the github repository above. \n",
        "\n",
        "All credits should be given to the original author."
      ],
      "metadata": {
        "id": "5OByERvcljfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "KKfWlnXSmCJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Null Values"
      ],
      "metadata": {
        "id": "RrzEAi3BmQT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Overall Distribution of Null Values"
      ],
      "metadata": {
        "id": "UPYPLwZZvViH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check to see if there are any missing values in our data set\n",
        "df.isnull().any()"
      ],
      "metadata": {
        "id": "O0G3F_2aH4_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gives some infomation on columns types and number of null values\n",
        "tab_info=pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column type'})\n",
        "tab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:'null values (nb)'}))\n",
        "tab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()/df_initial.shape[0]*100).T.\n",
        "                         rename(index={0:'null values (%)'}))\n",
        "print ('-' * 10 + \" Display information about column types and number of null values \" + '-' * 10 )\n",
        "print \n",
        "display(tab_info)"
      ],
      "metadata": {
        "id": "BIfH6fGIv2V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Investigate Why there is missing value\n",
        "\n",
        "After viewing the distribution of the null values, before deleting them, it might be a good idea to ask the following questions:\n",
        "\n",
        "1. Why are they missing? Are they missing for valid reasons or are they missing because of some unexpected bugs in the code?\n",
        "\n",
        "2. How are they missing? Are they missing at random or is missing correlated with some features?\n",
        "\n",
        "Possible ways to go:\n",
        "\n",
        "1. Drop them if the reason for missing is valid and there is nothing that you can do.\n",
        "2. Impute them using some statistical methods (personally not recommend)\n",
        "3. Encode a binary variable that shows the missing status"
      ],
      "metadata": {
        "id": "Gqc-w2_pwOXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop NA values that appeared in a particular column\n",
        "df_initial.dropna(axis = 0, subset = ['CustomerID'], inplace = True)"
      ],
      "metadata": {
        "id": "STYbFlpswNsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Duplicates"
      ],
      "metadata": {
        "id": "WNfkD8-jm4t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Duplicate data entries: {}'.format(df_initial.duplicated().sum()))\n",
        "df_initial.drop_duplicates(inplace = True)"
      ],
      "metadata": {
        "id": "fXRA701tySE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Outliers"
      ],
      "metadata": {
        "id": "8aYuGAqqA4g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# box plot\n",
        "sns.set(font_scale=1.0)\n",
        "fig, axes = plt.subplots(nrows=3,ncols=2)\n",
        "fig.set_size_inches(15, 15)\n",
        "sns.boxplot(data=train,y=\"cnt\",orient=\"v\",ax=axes[0][0])\n",
        "sns.boxplot(data=train,y=\"cnt\",x=\"mnth\",orient=\"v\",ax=axes[0][1])\n",
        "sns.boxplot(data=train,y=\"cnt\",x=\"weathersit\",orient=\"v\",ax=axes[1][0])\n",
        "sns.boxplot(data=train,y=\"cnt\",x=\"workingday\",orient=\"v\",ax=axes[1][1])\n",
        "sns.boxplot(data=train,y=\"cnt\",x=\"hr\",orient=\"v\",ax=axes[2][0])\n",
        "sns.boxplot(data=train,y=\"cnt\",x=\"temp\",orient=\"v\",ax=axes[2][1])\n",
        "\n",
        "axes[0][0].set(ylabel='Count',title=\"Box Plot On Count\")\n",
        "axes[0][1].set(xlabel='Month', ylabel='Count',title=\"Box Plot On Count Across Months\")\n",
        "axes[1][0].set(xlabel='Weather Situation', ylabel='Count',title=\"Box Plot On Count Across Weather Situations\")\n",
        "axes[1][1].set(xlabel='Working Day', ylabel='Count',title=\"Box Plot On Count Across Working Day\")\n",
        "axes[2][0].set(xlabel='Hour Of The Day', ylabel='Count',title=\"Box Plot On Count Across Hour Of The Day\")\n",
        "axes[2][1].set(xlabel='Temperature', ylabel='Count',title=\"Box Plot On Count Across Temperature\")"
      ],
      "metadata": {
        "id": "GIrG_idwA7wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# standardization or IQR\n",
        "q1 = train.cnt.quantile(0.25)\n",
        "q3 = train.cnt.quantile(0.75)\n",
        "iqr = q3 - q1\n",
        "lower_bound = q1 -(1.5 * iqr) \n",
        "upper_bound = q3 +(1.5 * iqr) \n",
        "train_preprocessed = train.loc[(train.cnt >= lower_bound) & (train.cnt <= upper_bound)]\n",
        "print(\"Samples in train set without outliers: {}\".format(len(train_preprocessed)))"
      ],
      "metadata": {
        "id": "ZWt1JbhnA__D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Restructing Data"
      ],
      "metadata": {
        "id": "s4RGXRVcCyDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the reponse variable \"turnover\" to the front of the table\n",
        "front = df['turnover']\n",
        "df.drop(labels=['turnover'], axis=1,inplace = True)\n",
        "df.insert(0, 'turnover', front)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bPgCyadPIE26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# customer visit, time series construct\n",
        "\n",
        "# weekday\n",
        "def add_day_of_week(index): \n",
        "    return (index) % 7\n",
        "\n",
        "df['day_of_week'] = df['visit_day'].apply(add_day_of_week) # add new column for day of week\n",
        "df.loc[df['day_of_week'] == 0, 'day_of_week'] = 7 # set all the 0's to 7 (Sunday)\n",
        "df['day_of_week'] = df['day_of_week'].astype(np.uint8) # convert to uint8 to save memory"
      ],
      "metadata": {
        "id": "-PM1Y86CC410"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape wide to long\n",
        "df_raw1 = (df_raw.merge(visits_df, right_index = True, left_index = True) # merge the original data with the expanded visits\n",
        "                .drop(['visits'], axis=1) # drop the origil visits columns\n",
        "                .melt(id_vars = ['visitor_id'], value_name = \"visit_day\") # transform from wide format to long format\n",
        "                .drop(\"variable\", axis = 1) # drop variable column which was added by melt function\n",
        "                .dropna() # drop the missing entries\n",
        "            )"
      ],
      "metadata": {
        "id": "o89Kqeq-T2R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shift data period\n",
        "features_tot_dow = list(df_w2.columns[df_w2.columns.str.contains('tot_dow')].values)\n",
        "features_prop_dow = list(df_w2.columns[df_w2.columns.str.contains('prop_dow')].values)\n",
        "features_other = ['freq', 'weeks_since_prev_visit']\n",
        "features = []\n",
        "features.extend(features_tot_dow)\n",
        "features.extend(features_prop_dow)\n",
        "features.extend(features_other)\n",
        "features\n",
        "\n",
        "df_w2[features] = df_w2.groupby('visitor_id')[features].shift(1)"
      ],
      "metadata": {
        "id": "V3y6mXNbeGO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration\n",
        "\n",
        "Remember to explore the constucted features after feature engineering step"
      ],
      "metadata": {
        "id": "zyXehNlPnH99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Data Attributes"
      ],
      "metadata": {
        "id": "fpUBfTl2nN8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Some Commonly Used Commands"
      ],
      "metadata": {
        "id": "XjHGWjxKhIwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# view 20 rows randomly sampled from the dataframe\n",
        "df.sample(20)\n",
        "\n",
        "# view 20 columns randomly sampled from the dataframe\n",
        "df.nlargest(10,\"age\")\n",
        "\n",
        "# sorting\n",
        "df.sort_value(\"age\",ascending=False).head(10)\n",
        "\n",
        "df.info()\n",
        "\n",
        "# obtain dimension\n",
        "df.shape()\n",
        "\n",
        "df.describe().T\n",
        "\n",
        "# find the counts of NA\n",
        "df.isna().sum()\n",
        "\n",
        "# find unique value\n",
        "df.nunique()\n",
        "\n",
        "# string filter\n",
        "df.name.str.contains(\"abc\")\n",
        "\n",
        "# select columns of a dataframe\n",
        "df[[\"age\",\"income\"]]\n",
        "\n",
        "df.query('city==\"SH\"')\n",
        "\n",
        "df.sort_values(by=[\"city\",\"age\"])\n",
        "\n",
        "np.where(condition,\"Y\",\"N\")\n",
        "\n",
        "# string operation\n",
        "df.location.str.strip()\n",
        "\n",
        "df.location.str.lstrip()\n",
        "\n",
        "df.location.str.rstrip()\n",
        "\n",
        "df.city.replace('A','B')\n",
        "\n",
        "df.location.str.lower()\n",
        "\n",
        "# dataframe merge/join\n",
        "pd.merge(df1,df2,how=\"left\",left_on=\"\",right_on=\"\")"
      ],
      "metadata": {
        "id": "E6-DZOc5hTRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (Number of) Unique Values of a column"
      ],
      "metadata": {
        "id": "r4ybSDe9nYrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show unique values\n",
        "df_initial['Country'].value_counts()\n",
        "\n",
        "# show how many unique values\n",
        "len(df_initial['Country'].value_counts())\n",
        "print('No. of cuntries in dataframe: {}'.format(len(df_initial['Country'].value_counts())))"
      ],
      "metadata": {
        "id": "n_A63T0L0Kna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Aggregation by Some Attribute"
      ],
      "metadata": {
        "id": "Qg9-v0HW0lN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# aggregate by country id, count the number of orders/customers\n",
        "temp_no_of_order_per_count = df_initial[['CustomerID','Country']].groupby(['Country']).count()\n",
        "temp_no_of_order_per_count = temp_no_of_order_per_count.reset_index(drop = False)"
      ],
      "metadata": {
        "id": "zdHDl6Zb03Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the number of products purchased in every transaction\n",
        "temp = df_initial.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate'].count()"
      ],
      "metadata": {
        "id": "cmgJQ0vY1z1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# order cancel rate\n",
        "nb_products_per_basket['order_cancelled'] = nb_products_per_basket['InvoiceNo'].apply(\n",
        "    lambda x:int('C' in x))\n",
        "\n",
        "n1 = nb_products_per_basket['order_cancelled'].sum()\n",
        "n2 = nb_products_per_basket.shape[0]\n",
        "percentage = (n1/n2)*100\n",
        "print('Number of orders cancelled: {}/{} ({:.2f}%) '.format(n1, n2, percentage))"
      ],
      "metadata": {
        "id": "ea_RMP3C3UN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# of visits and stats on cart amount / users\n",
        "transactions_per_user=basket_price.groupby(by=['CustomerID'])['Basket Price'].agg(['count','min','max','mean','sum'])"
      ],
      "metadata": {
        "id": "L2n7kMfB9oGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accumulated sum\n",
        "df_w1['freq'] = df_w1.groupby('visitor_id')['total_visits_in_week'].cumsum().astype(np.uint32)"
      ],
      "metadata": {
        "id": "klSUsMrOdqZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pivot, customer segmentation check"
      ],
      "metadata": {
        "id": "2sKMIdUl-pzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation Analysis"
      ],
      "metadata": {
        "id": "lOGMy9LxBSYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = train[number_features + target].corr()\n",
        "heat = np.array(matrix)\n",
        "heat[np.tril_indices_from(heat)] = False\n",
        "fig,ax= plt.subplots()\n",
        "fig.set_size_inches(20,10)\n",
        "sns.set(font_scale=1.0)\n",
        "sns.heatmap(matrix, mask=heat,vmax=1.0, vmin=0.0, square=True,annot=True, cmap=\"Reds\")"
      ],
      "metadata": {
        "id": "H4W8dv_1BVqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering"
      ],
      "metadata": {
        "id": "BqS2lFAn-ONL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature added using existing columns"
      ],
      "metadata": {
        "id": "Y2NfCv-7_CBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the number of days elapsed since the first purchase (** FirstPurchase ) \n",
        "# and the number of days since the last purchase ( LastPurchase **)\n",
        "last_date = basket_price['InvoiceDate'].max().date()\n",
        "\n",
        "first_registration = pd.DataFrame(basket_price.groupby(by=['CustomerID'])['InvoiceDate'].min())\n",
        "last_purchase      = pd.DataFrame(basket_price.groupby(by=['CustomerID'])['InvoiceDate'].max())\n",
        "\n",
        "test  = first_registration.applymap(lambda x:(last_date - x.date()).days)\n",
        "test2 = last_purchase.applymap(lambda x:(last_date - x.date()).days)\n",
        "\n",
        "transactions_per_user.loc[:, 'LastPurchase'] = test2.reset_index(drop = False)['InvoiceDate']\n",
        "transactions_per_user.loc[:, 'FirstPurchase'] = test.reset_index(drop = False)['InvoiceDate']\n"
      ],
      "metadata": {
        "id": "uGLaL3lT-a9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature added using other machine learning tools"
      ],
      "metadata": {
        "id": "gToPqnQM_GfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# customer segmentation, creation of customer clusters"
      ],
      "metadata": {
        "id": "2XqiPu0A_WCP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}